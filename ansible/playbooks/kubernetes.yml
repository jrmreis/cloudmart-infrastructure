---
# Main Kubernetes deployment playbook for EKS
# Updated with wait conditions between phases for AWS managed Kubernetes

- name: Configure EKS control plane
  hosts: localhost
  connection: local
  gather_facts: yes
  vars:
    cluster_name: "cloudmart"
    aws_region: "us-east-1"
    dynamodb_table_name: "cloudmart-products"
    ec2_username: "ec2-user"
  tasks:
    - name: Check if EKS cluster exists
      shell: |
        aws eks list-clusters --region {{ aws_region }} | grep {{ cluster_name }} || echo "Cluster not found"
      register: cluster_exists
      changed_when: false
      ignore_errors: yes
      
    - name: Create EKS cluster with nodegroup
      shell: |
        eksctl create cluster \
          --name {{ cluster_name }} \
          --region {{ aws_region }} \
          --nodegroup-name standard-workers \
          --node-type t3.medium \
          --nodes 2 \
          --nodes-min 1 \
          --nodes-max 3 \
          --managed
      args:
        executable: /bin/bash
        chdir: "/home/{{ ec2_username }}/k8s"
      become_user: "{{ ec2_username }}"
      register: eks_create
      async: 1800  # 30 minutes
      poll: 0
      when: "'Cluster not found' in cluster_exists.stdout"
      
    # Save the async job ID to a variable that won't get lost
    - name: Save async job ID
      set_fact:
        async_job_id: "{{ eks_create.ansible_job_id }}"
      when: eks_create is defined and eks_create.ansible_job_id is defined

- name: Verify EKS control plane is ready
  hosts: localhost
  connection: local
  gather_facts: no
  vars:
    cluster_name: "cloudmart"
    aws_region: "us-east-1"
  tasks:
    - name: Check EKS cluster creation status
      async_status:
        jid: "{{ async_job_id | default('') }}"
      register: job_result
      until: job_result.finished | default(false)
      retries: 60
      delay: 30
      # Remove async tracking if job is no longer valid
      ignore_errors: yes
      when: async_job_id is defined and async_job_id != ""

    - name: Wait for EKS cluster to exist
      shell: |
        aws eks list-clusters --region {{ aws_region }} | grep {{ cluster_name }} || echo "Cluster not found"
      register: cluster_exists_check
      until: cluster_name in cluster_exists_check.stdout
      retries: 30
      delay: 20
      ignore_errors: yes

    # Wait even if previous check passes since the cluster might exist but not be ACTIVE
    - name: Wait for EKS cluster to be active
      shell: |
        aws eks describe-cluster --name {{ cluster_name }} --region {{ aws_region }} --query 'cluster.status' || echo "CREATING"
      register: cluster_status
      until: "'ACTIVE' in cluster_status.stdout"
      retries: 30
      delay: 20
      ignore_errors: yes
      
    - name: Update kubectl configuration
      shell: aws eks update-kubeconfig --name {{ cluster_name }} --region {{ aws_region }}
      args:
        executable: /bin/bash
      register: kubeconfig_update
      ignore_errors: yes
      
    - name: Get EKS API endpoint
      shell: |
        aws eks describe-cluster --name {{ cluster_name }} --region {{ aws_region }} --query 'cluster.endpoint' --output text
      register: eks_endpoint
      when: cluster_status.stdout is defined and 'ACTIVE' in cluster_status.stdout
      
    - name: Wait for API server to be fully available
      shell: |
        kubectl get --raw /healthz
      register: api_status
      until: api_status.rc == 0
      retries: 30
      delay: 10
      ignore_errors: yes

- name: Verify Networking Components
  hosts: localhost
  connection: local
  gather_facts: no
  vars:
    aws_region: "us-east-1"
  tasks:    
    - name: Wait for CoreDNS to be ready
      shell: |
        kubectl get pods -n kube-system -l k8s-app=kube-dns -o jsonpath='{.items[*].status.phase}' | grep -v Running || echo "CoreDNS Ready"
      register: coredns_status
      until: "'CoreDNS Ready' in coredns_status.stdout or 'Running' in coredns_status.stdout"
      retries: 30
      delay: 10
      ignore_errors: yes

- name: Verify Node Readiness
  hosts: localhost
  connection: local
  gather_facts: no
  vars:
    cluster_name: "cloudmart"
    aws_region: "us-east-1"
  tasks:
    - name: Wait for EKS nodegroup to be active
      shell: |
        aws eks list-nodegroups --cluster-name {{ cluster_name }} --region {{ aws_region }} | grep standard-workers || echo "No nodegroups found"
      register: nodegroup_exists
      until: "'standard-workers' in nodegroup_exists.stdout"
      retries: 20
      delay: 15
      ignore_errors: yes
      
    - name: Wait for nodes to join and be ready
      shell: |
        kubectl get nodes | grep NotReady || echo "All nodes ready"
      register: nodes_ready
      until: "'All nodes ready' in nodes_ready.stdout"
      retries: 30
      delay: 10
      ignore_errors: yes
      
    - name: Get node details
      shell: kubectl get nodes
      register: node_status
      ignore_errors: yes
      
    - name: Display node status
      debug:
        var: node_status.stdout_lines
      when: node_status.stdout is defined

- name: Deploy CloudMart Applications
  hosts: localhost
  connection: local
  gather_facts: no
  vars:
    aws_region: "us-east-1"
    ec2_username: "ec2-user"
  tasks:
    - name: Create deployment directory
      file:
        path: "/home/{{ ec2_username }}/k8s"
        state: directory
        mode: '0755'
        owner: "{{ ec2_username }}"
        group: "{{ ec2_username }}"
      
    - name: Create backend deployment file
      template:
        src: templates/backend-deployment.yaml.j2
        dest: "/home/{{ ec2_username }}/k8s/backend-deployment.yaml"
        owner: "{{ ec2_username }}"
        group: "{{ ec2_username }}"
        mode: '0644'
      
    - name: Create frontend deployment file
      template:
        src: templates/frontend-deployment.yaml.j2
        dest: "/home/{{ ec2_username }}/k8s/frontend-deployment.yaml"
        owner: "{{ ec2_username }}"
        group: "{{ ec2_username }}"
        mode: '0644'
      
    - name: Apply backend deployment
      shell: kubectl apply -f /home/{{ ec2_username }}/k8s/backend-deployment.yaml
      args:
        executable: /bin/bash
      register: backend_deployment
      
    # Add wait for backend deployment to be ready
    - name: Wait for backend deployment to be ready
      shell: |
        kubectl wait --for=condition=available deployment/cloudmart-backend --timeout=120s || echo "Backend deployment not ready yet"
      args:
        executable: /bin/bash
      register: backend_ready
      retries: 5
      delay: 15
      until: backend_ready.rc == 0 or "Backend deployment not ready yet" in backend_ready.stdout
      ignore_errors: yes
      
    - name: Apply frontend deployment
      shell: kubectl apply -f /home/{{ ec2_username }}/k8s/frontend-deployment.yaml
      args:
        executable: /bin/bash
      register: frontend_deployment
      
    - name: Wait for services to be provisioned (load balancers take time)
      pause:
        seconds: 60
        prompt: "Waiting for Kubernetes services to be provisioned..."

    # Add wait for all pods to be running
    - name: Wait for all pods to be running
      shell: |
        kubectl get pods --no-headers | grep -v Running | grep -v Completed || echo "All pods running"
      register: pods_status
      until: "'All pods running' in pods_status.stdout"
      retries: 10
      delay: 15
      args:
        executable: /bin/bash
      ignore_errors: yes
      
    - name: Get Kubernetes services status
      shell: kubectl get services
      args:
        executable: /bin/bash
      register: k8s_services
      changed_when: false
      
    - name: Get Kubernetes pods status
      shell: kubectl get pods
      args:
        executable: /bin/bash
      register: k8s_pods
      changed_when: false
      
    - name: Log Kubernetes deployments status
      copy:
        dest: "/home/{{ ec2_username }}/logs/kubernetes_deployment_{{ ansible_date_time.date }}.txt"
        content: |
          Backend Deployment: {{ backend_deployment.stdout | default('Not available') }}
          Frontend Deployment: {{ frontend_deployment.stdout | default('Not available') }}
          Kubernetes Services: {{ k8s_services.stdout | default('Not available') }}
          Kubernetes Pods: {{ k8s_pods.stdout | default('Not available') }}
        owner: "{{ ec2_username }}"
        group: "{{ ec2_username }}"
        mode: '0644'
