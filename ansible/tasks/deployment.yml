---
# deployment.yml - Tasks for creating and deploying to the EKS cluster

- name: Set default AWS variables if not defined
  set_fact:
    dynamodb_table_name: "{{ dynamodb_table_name | default('cloudmart-products') }}"
    aws_region: "{{ aws_region | default('us-east-1') }}"
    cluster_name: "{{ cluster_name | default('cloudmart') }}"

- name: Create IAM policy for DynamoDB access
  shell: |
    aws iam create-policy \
      --policy-name EKSNodeDynamoDBPolicy \
      --policy-document '{
        "Version": "2012-10-17",
        "Statement": [
          {
            "Effect": "Allow",
            "Action": [
              "dynamodb:Scan",
              "dynamodb:GetItem",
              "dynamodb:PutItem",
              "dynamodb:UpdateItem", 
              "dynamodb:DeleteItem",
              "dynamodb:Query"
            ],
            "Resource": "arn:aws:dynamodb:{{ aws_region }}:*:table/{{ dynamodb_table_name }}"
          }
        ]
      }'
  register: policy_result
  failed_when: policy_result.rc != 0 and 'EntityAlreadyExists' not in policy_result.stderr
  changed_when: policy_result.rc == 0
  ignore_errors: yes

- name: Attach DynamoDB policy to EKS node role
  shell: |
    NODE_ROLE=$(aws eks describe-nodegroup --cluster-name {{ cluster_name }} --nodegroup-name standard-workers --region {{ aws_region }} --query 'nodegroup.nodeRole' --output text | awk -F '/' '{print $2}' | awk -F ',' '{print $1}')
    POLICY_ARN=$(aws iam list-policies --query 'Policies[?PolicyName==`EKSNodeDynamoDBPolicy`].Arn' --output text)
    aws iam attach-role-policy --role-name $NODE_ROLE --policy-arn $POLICY_ARN
  register: attach_policy
  changed_when: attach_policy.rc == 0
  ignore_errors: yes

- name: Wait for DynamoDB table to be active
  shell: |
    aws dynamodb describe-table --table-name {{ dynamodb_table_name }} --region {{ aws_region }} --query 'Table.TableStatus' | grep ACTIVE || echo "Table not ready"
  register: dynamodb_status
  until: "'ACTIVE' in dynamodb_status.stdout"
  retries: 15
  delay: 10
  ignore_errors: yes

# Check if cluster already exists before trying to create it
- name: Check if EKS cluster already exists
  shell: |
    aws eks list-clusters --region {{ aws_region }} | grep {{ cluster_name }} || echo "Cluster not found"
  register: cluster_exists
  changed_when: false
  ignore_errors: yes

- name: Create EKS cluster with nodegroup (this will take ~15 minutes)
  shell: |
    eksctl create cluster \
      --name {{ cluster_name }} \
      --region {{ aws_region }} \
      --nodegroup-name standard-workers \
      --node-type t3.medium \
      --nodes 1 \
      --nodes-min 1 \
      --nodes-max 3 \
      --managed
  args:
    executable: /bin/bash
    chdir: "/home/ec2-user/k8s"
  become_user: "{{ ec2_username }}"
  register: eks_create
  # Use async to prevent Ansible timeout during cluster creation
  async: 1800  # 30 minutes
  poll: 0
  when: '"Cluster not found" in cluster_exists.stdout'
  
# Save the async job ID to a variable that won't get lost
- name: Save async job ID
  set_fact:
    async_job_id: "{{ eks_create.ansible_job_id }}"
  when: eks_create is defined and eks_create.ansible_job_id is defined

- name: Check EKS cluster creation status
  async_status:
    jid: "{{ async_job_id | default('') }}"
  register: job_result
  until: job_result.finished | default(false)
  retries: 60
  delay: 30
  # Remove async tracking if job is no longer valid
  ignore_errors: yes
  when: async_job_id is defined and async_job_id != ""

- name: Debug Async Job Status
  debug:
    var: job_result
  when: job_result is defined

# Explicitly wait for EKS cluster to exist and be active
- name: Wait for EKS cluster to exist
  shell: |
    aws eks list-clusters --region {{ aws_region }} | grep {{ cluster_name }} || echo "Cluster not found"
  register: cluster_exists_check
  until: cluster_name in cluster_exists_check.stdout
  retries: 30
  delay: 20
  ignore_errors: yes

# Wait even if previous check passes since the cluster might exist but not be ACTIVE
- name: Wait for EKS cluster to be active
  shell: |
    aws eks describe-cluster --name {{ cluster_name }} --region {{ aws_region }} --query 'cluster.status' || echo "CREATING"
  register: cluster_status
  until: "'ACTIVE' in cluster_status.stdout"
  retries: 30
  delay: 20
  ignore_errors: yes
  
- name: Print Cluster Status
  debug:
    var: cluster_status.stdout
  when: cluster_status is defined and cluster_status.stdout is defined
  
- name: Update kubectl configuration
  shell: aws eks update-kubeconfig --name {{ cluster_name }} --region {{ aws_region }}
  args:
    executable: /bin/bash
  become_user: "{{ ec2_username }}"
  register: kubeconfig_update
  ignore_errors: yes
  
# Explicit pause for IAM configurations to propagate
- name: Allow time for IAM configurations to propagate after configuring kubectl
  pause:
    seconds: 30
  when: kubeconfig_update is not failed
  
- name: Associate IAM OIDC Provider with EKS Cluster
  shell: |
    eksctl utils associate-iam-oidc-provider \
      --region={{ aws_region }} \
      --cluster={{ cluster_name }} \
      --approve
  register: oidc_association
  changed_when: "'associated' in oidc_association.stdout"
  failed_when: false
  ignore_errors: yes

# Add crucial pause for OIDC provider association to propagate
- name: Allow time for OIDC provider association to propagate
  pause:
    seconds: 45
  when: oidc_association is defined and not oidc_association.failed

- name: Create IAM Service Account for CloudMart Pods
  shell: |
    eksctl create iamserviceaccount \
      --cluster={{ cluster_name }} \
      --name=cloudmart-pod-execution-role \
      --role-name=CloudMartPodExecutionRole \
      --attach-policy-arn=arn:aws:iam::aws:policy/AdministratorAccess \
      --region={{ aws_region }} \
      --approve
  register: service_account_creation
  changed_when: "'created' in service_account_creation.stdout"
  failed_when: false
  ignore_errors: yes

# Add critical pause for IAM role propagation
- name: Allow time for IAM role propagation
  pause:
    seconds: 60
  when: service_account_creation is defined and not service_account_creation.failed

- name: Debug IAM Service Account Creation
  debug:
    var: service_account_creation
  when: service_account_creation.stdout is defined

# Add DynamoDB connectivity test
- name: Test DynamoDB connectivity from kubernetes
  shell: |
    kubectl run aws-cli --rm -i --tty=false --restart=Never --image=amazon/aws-cli -- aws dynamodb list-tables --region {{ aws_region }}
  register: dynamodb_connectivity
  until: dynamodb_connectivity.rc == 0
  retries: 5
  delay: 10
  ignore_errors: yes
  
- name: Create backend deployment file
  template:
    src: templates/backend-deployment.yaml.j2
    dest: /home/ec2-user/k8s/backend-deployment.yaml
    owner: "{{ ec2_username }}"
    group: "{{ ec2_username }}"
    mode: '0644'
    
- name: Create frontend deployment file
  template:
    src: templates/frontend-deployment.yaml.j2
    dest: /home/ec2-user/k8s/frontend-deployment.yaml
    owner: "{{ ec2_username }}"
    group: "{{ ec2_username }}"
    mode: '0644'
    
- name: Apply backend deployment
  shell: kubectl apply -f /home/ec2-user/k8s/backend-deployment.yaml
  args:
    executable: /bin/bash
  become_user: "{{ ec2_username }}"
  register: backend_deployment
  
# Add wait for backend deployment to be ready
- name: Wait for backend deployment to be ready
  shell: |
    kubectl wait --for=condition=available deployment/cloudmart-backend --timeout=120s || echo "Backend deployment not ready yet"
  args:
    executable: /bin/bash
  become_user: "{{ ec2_username }}"
  register: backend_ready
  retries: 5
  delay: 15
  until: backend_ready.rc == 0 or "Backend deployment not ready yet" in backend_ready.stdout
  ignore_errors: yes
  
- name: Apply frontend deployment
  shell: kubectl apply -f /home/ec2-user/k8s/frontend-deployment.yaml
  args:
    executable: /bin/bash
  become_user: "{{ ec2_username }}"
  register: frontend_deployment
  
- name: Wait for services to be provisioned (load balancers take time)
  pause:
    seconds: 60
    prompt: "Waiting for Kubernetes services to be provisioned..."

# Add wait for all pods to be running
- name: Wait for all pods to be running
  shell: |
    kubectl get pods --no-headers | grep -v Running | grep -v Completed || echo "All pods running"
  register: pods_status
  until: "'All pods running' in pods_status.stdout"
  retries: 10
  delay: 15
  args:
    executable: /bin/bash
  become_user: "{{ ec2_username }}"
  ignore_errors: yes
    
- name: Get Kubernetes services status
  shell: kubectl get services
  args:
    executable: /bin/bash
  become_user: "{{ ec2_username }}"
  register: k8s_services
  changed_when: false
  
- name: Log Kubernetes deployments status
  copy:
    dest: "{{ log_dir }}/kubernetes_deployment_{{ ansible_date_time.date }}.txt"
    content: |
      Backend Deployment: {{ backend_deployment.stdout | default('Not available') }}
      Frontend Deployment: {{ frontend_deployment.stdout | default('Not available') }}
      Kubernetes Services: {{ k8s_services.stdout | default('Not available') }}
    owner: "{{ ec2_username }}"
    group: "{{ ec2_username }}"
    mode: '0644'
