---
# Tasks for creating and deploying to the EKS cluster

- name: Create IAM verification script
  template:
    src: verify-iam.sh.j2
    dest: "{{ k8s_dir }}/verify-iam.sh"
    owner: "{{ ec2_username }}"
    group: "{{ ec2_username }}"
    mode: '0755'
  tags: [deployment, iam]

- name: Verify IAM permissions
  shell: |
    {{ k8s_dir }}/verify-iam.sh
  args:
    executable: /bin/bash
  register: iam_verification
  ignore_errors: yes
  tags: [deployment, iam]

- name: Create EKS cluster (this will take ~15 minutes)
  shell: |
    export AWS_REGION={{ aws_region }}
    VPC_ID=$(aws ec2 describe-vpcs --filters "Name=isDefault,Values=true" --query "Vpcs[0].VpcId" --output text)
    SUBNET_IDS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query "Subnets[*].SubnetId" --output text | tr '\t' ',')
    
    eksctl create cluster \
      --name {{ cluster_name }} \
      --region {{ aws_region }} \
      --nodegroup-name standard-workers \
      --node-type {{ eks_node_type }} \
      --nodes {{ eks_nodes }} \
      --with-oidc \
      --managed || true
  environment:
    AWS_REGION: "{{ aws_region }}"
  register: create_cluster
  args:
    executable: /bin/bash
  async: 2700  # 45 minutes
  poll: 0
  tags: [deployment, eks]

- name: Check EKS cluster creation status
  shell: |
    export AWS_REGION={{ aws_region }}
    eksctl get cluster --name {{ cluster_name }} --region {{ aws_region }} || echo "Cluster creation in progress"
  environment:
    AWS_REGION: "{{ aws_region }}"
  register: cluster_status
  until: "'{{ cluster_name }}' in cluster_status.stdout and 'ACTIVE' in cluster_status.stdout"
  retries: 30
  delay: 120  # Check every 2 minutes
  ignore_errors: yes
  tags: [deployment, eks]

- name: Create cluster health check script
  template:
    src: cluster-health-check.sh.j2
    dest: "{{ k8s_dir }}/cluster-health-check.sh"
    owner: "{{ ec2_username }}"
    group: "{{ ec2_username }}"
    mode: '0755'
  tags: [deployment, eks, health]

- name: Get EKS cluster details
  shell: |
    aws eks describe-cluster --name {{ cluster_name }} --region {{ aws_region }} --query "cluster" --output json
  register: eks_cluster_details
  ignore_errors: yes
  tags: [deployment, eks, health]

- name: Parse EKS cluster status
  set_fact:
    eks_cluster_status: "{{ (eks_cluster_details.stdout | from_json).status | default('UNKNOWN') }}"
    eks_cluster_endpoint: "{{ (eks_cluster_details.stdout | from_json).endpoint | default('') }}"
    eks_last_check_time: "{{ ansible_date_time.iso8601 }}"
  ignore_errors: yes
  tags: [deployment, eks, health]

- name: Set deployment readiness based on cluster status
  set_fact:
    cluster_is_ready: "{{ eks_cluster_status == 'ACTIVE' }}"
  tags: [deployment, eks, health]

- name: Create health status file
  copy:
    dest: "{{ k8s_dir }}/cluster_health.json"
    content: |
      {
        "cluster_name": "{{ cluster_name }}",
        "status": "{{ eks_cluster_status }}",
        "endpoint": "{{ eks_cluster_endpoint }}",
        "last_check": "{{ eks_last_check_time }}",
        "deployment_ready": {{ cluster_is_ready | to_json }}
      }
    owner: "{{ ec2_username }}"
    group: "{{ ec2_username }}"
    mode: '0644'
  tags: [deployment, eks, health]

- name: Update kubectl configuration
  shell: aws eks update-kubeconfig --name {{ cluster_name }} --region {{ aws_region }}
  register: kubeconfig_update
  ignore_errors: yes
  failed_when: false
  tags: [deployment, eks]
  
- name: Associate IAM OIDC Provider with EKS Cluster
  shell: |
    eksctl utils associate-iam-oidc-provider \
      --region={{ aws_region }} \
      --cluster={{ cluster_name }} \
      --approve
  register: oidc_association
  changed_when: "'associated' in oidc_association.stdout"
  failed_when: oidc_association.rc != 0
  tags: [deployment, eks, iam]

- name: Create IAM Service Account for CloudMart Pods
  shell: |
    eksctl create iamserviceaccount \
      --cluster={{ cluster_name }} \
      --name=cloudmart-pod-execution-role \
      --role-name=CloudMartPodExecutionRole \
      --attach-policy-arn=arn:aws:iam::aws:policy/AdministratorAccess \
      --region={{ aws_region }} \
      --approve
  register: service_account_creation
  changed_when: "'created' in service_account_creation.stdout"
  failed_when: service_account_creation.rc != 0
  tags: [deployment, eks, iam]

- name: Debug IAM Service Account Creation
  debug:
    var: service_account_creation
  when: service_account_creation.stdout is defined
  tags: [deployment, eks, iam, debug]

- name: Apply kubectl context with retry
  shell: |
    export AWS_REGION={{ aws_region }}
    aws eks update-kubeconfig --name {{ cluster_name }} --region {{ aws_region }}
    # Wait for DNS to propagate
    for i in {1..10}; do
      kubectl get nodes && break
      echo "Waiting for API server to be reachable (attempt $i)..."
      sleep 30
    done
  environment:
    AWS_REGION: "{{ aws_region }}"
    KUBECONFIG: "/home/{{ ec2_username }}/.kube/config"
  args:
    executable: /bin/bash
  register: kubectl_context_result
  ignore_errors: yes
  tags: [deployment, kubernetes]
  
- name: Create backend deployment file
  template:
    src: backend-deployment.yaml.j2
    dest: "{{ k8s_dir }}/backend-deployment.yaml"
    owner: "{{ ec2_username }}"
    group: "{{ ec2_username }}"
    mode: '0644'
  tags: [deployment, kubernetes]
    
- name: Create frontend deployment file
  template:
    src: frontend-deployment.yaml.j2
    dest: "{{ k8s_dir }}/frontend-deployment.yaml"
    owner: "{{ ec2_username }}"
    group: "{{ ec2_username }}"
    mode: '0644'
  tags: [deployment, kubernetes]

- name: Create symbolic link for kubectl if needed
  file:
    src: /home/{{ ec2_username }}/bin/kubectl
    dest: /usr/bin/kubectl
    state: link
    force: yes
  tags: [deployment, kubernetes]
    
- name: Apply backend deployment
  shell: |
    kubectl apply -f "{{ k8s_dir }}/backend-deployment.yaml"
  args:
    executable: /bin/bash
  register: backend_deployment
  ignore_errors: yes
  tags: [deployment, kubernetes]

- name: Apply frontend deployment
  shell: |
    kubectl apply -f "{{ k8s_dir }}/frontend-deployment.yaml"
  args:
    executable: /bin/bash
  register: frontend_deployment
  ignore_errors: yes
  tags: [deployment, kubernetes]
  
- name: Wait for services to be provisioned (load balancers take time)
  pause:
    seconds: 60
    prompt: "Waiting for Kubernetes services to be provisioned..."
  tags: [deployment, kubernetes]
    
- name: Get Kubernetes services status
  shell: |
    export AWS_REGION={{ aws_region }}
    export KUBECONFIG=/home/{{ ec2_username }}/.kube/config
    # Try multiple times with increasing delays
    for i in {1..5}; do
      if kubectl get services; then
        break
      else
        echo "Attempt $i: Unable to connect to Kubernetes API server. Waiting longer..."
        sleep $((i * 20))
      fi
    done
  args:
    executable: /bin/bash
  register: k8s_services
  changed_when: false
  ignore_errors: yes
  tags: [deployment, kubernetes, status]

- name: Validate kubectl configuration
  shell: kubectl config current-context
  register: kubectl_context
  ignore_errors: yes
  changed_when: false
  tags: [deployment, kubernetes, validate]

- name: Display current kubectl context
  debug:
    var: kubectl_context.stdout
  when: kubectl_context is defined and kubectl_context.rc == 0
  tags: [deployment, kubernetes, validate]
  
- name: Log Kubernetes deployments status
  copy:
    dest: "{{ log_dir }}/kubernetes_deployment_{{ ansible_date_time.date }}.txt"
    content: |
      Backend Deployment: {{ backend_deployment.stdout | default('') }}
      Frontend Deployment: {{ frontend_deployment.stdout | default('') }}
      Kubernetes Services: {{ k8s_services.stdout | default('') }}
    owner: "{{ ec2_username }}"
    group: "{{ ec2_username }}"
    mode: '0644'
  tags: [deployment, kubernetes, logs]

- name: Update deployment ready status
  set_fact:
    eks_deployment_ready: "{{ eks_cluster_status == 'ACTIVE' }}"
  tags: [deployment, eks, health]

# After kubectl configuration is updated in your deployment role
- name: Apply CloudMart ConfigMap
  shell: |
    kubectl apply -f "{{ k8s_dir }}/cloudmart-config.yaml"
  args:
    executable: /bin/bash
  register: configmap_result
  ignore_errors: no
  tags: [deployment, kubernetes, config]

- name: Create deployment helper script
  template:
    src: deploy-when-ready.sh.j2
    dest: "{{ k8s_dir }}/deploy-when-ready.sh"
    owner: "{{ ec2_username }}"
    group: "{{ ec2_username }}"
    mode: '0755'
  tags: [deployment, eks, health]

- name: Final deployment status update
  copy:
    dest: "{{ k8s_dir }}/deployment_status.txt"
    content: |
      Deployment Status: {{ 'Ready' if eks_deployment_ready else 'Not Ready' }}
      Cluster Status: {{ eks_cluster_status }}
      Last Check Time: {{ eks_last_check_time }}
      
      If the deployment is not ready, you can:
      1. Check the cluster status: aws eks describe-cluster --name {{ cluster_name }} --region {{ aws_region }}
      2. Wait for the cluster to be active and then run: {{ k8s_dir }}/cluster-health-check.sh --wait
      3. Once active, manually apply Kubernetes manifests: kubectl apply -f {{ k8s_dir }}/backend-deployment.yaml
    owner: "{{ ec2_username }}"
    group: "{{ ec2_username }}"
    mode: '0644'
  tags: [deployment, eks, health]
